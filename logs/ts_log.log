2025-08-17T02:23:03,563 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-08-17T02:23:03,563 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-08-17T02:23:03,566 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-08-17T02:23:03,566 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-08-17T02:23:03,589 [INFO ] main org.pytorch.serve.util.TokenAuthorization - 
######
TorchServe now enforces token authorization by default.
This requires the correct token to be provided when calling an API.
Key file located at /home/ashishverma/Documents/phishserve/key_file.json
Check token authorization documenation for information: https://github.com/pytorch/serve/blob/master/docs/token_authorization_api.md 
######

2025-08-17T02:23:03,589 [INFO ] main org.pytorch.serve.util.TokenAuthorization - 
######
TorchServe now enforces token authorization by default.
This requires the correct token to be provided when calling an API.
Key file located at /home/ashishverma/Documents/phishserve/key_file.json
Check token authorization documenation for information: https://github.com/pytorch/serve/blob/master/docs/token_authorization_api.md 
######

2025-08-17T02:23:03,589 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-08-17T02:23:03,589 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-08-17T02:23:03,612 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-08-17T02:23:03,612 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-08-17T02:23:03,682 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages
Current directory: /home/ashishverma/Documents/phishserve
Temp directory: /tmp
Metrics config path: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 20
Max heap size: 7976 M
Python executable: /home/ashishverma/Documents/phishserve/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ashishverma/Documents/phishserve/artifacts
Initial Models: phishserve.mar
Log dir: /home/ashishverma/Documents/phishserve/logs
Metrics dir: /home/ashishverma/Documents/phishserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ashishverma/Documents/phishserve/artifacts
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-08-17T02:23:03,682 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages
Current directory: /home/ashishverma/Documents/phishserve
Temp directory: /tmp
Metrics config path: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 20
Max heap size: 7976 M
Python executable: /home/ashishverma/Documents/phishserve/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ashishverma/Documents/phishserve/artifacts
Initial Models: phishserve.mar
Log dir: /home/ashishverma/Documents/phishserve/logs
Metrics dir: /home/ashishverma/Documents/phishserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ashishverma/Documents/phishserve/artifacts
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-08-17T02:23:03,688 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-08-17T02:23:03,688 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-08-17T02:23:03,688 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: phishserve.mar
2025-08-17T02:23:03,688 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: phishserve.mar
2025-08-17T02:23:03,720 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model phishserve
2025-08-17T02:23:03,720 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model phishserve
2025-08-17T02:23:03,720 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model phishserve
2025-08-17T02:23:03,720 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model phishserve
2025-08-17T02:23:03,720 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model phishserve loaded.
2025-08-17T02:23:03,720 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model phishserve loaded.
2025-08-17T02:23:03,720 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: phishserve, count: 1
2025-08-17T02:23:03,720 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: phishserve, count: 1
2025-08-17T02:23:03,724 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-08-17T02:23:03,724 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:03,724 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-08-17T02:23:03,724 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:03,749 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-08-17T02:23:03,749 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-08-17T02:23:03,749 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-08-17T02:23:03,749 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-08-17T02:23:03,750 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-08-17T02:23:03,750 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-08-17T02:23:03,750 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-08-17T02:23:03,750 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-08-17T02:23:03,750 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-08-17T02:23:03,750 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-08-17T02:23:03,852 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:23:03,852 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:23:03,885 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:23:03,885 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:23:04,263 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:23:04,264 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 15, in <module>
2025-08-17T02:23:04,264 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.async_service import AsyncService
2025-08-17T02:23:04,264 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/async_service.py", line 14, in <module>
2025-08-17T02:23:04,265 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.handler_utils.utils import create_predict_response
2025-08-17T02:23:04,265 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/handler_utils/utils.py", line 5, in <module>
2025-08-17T02:23:04,265 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-08-17T02:23:04,265 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/protocol/otf_message_handler.py", line 16, in <module>
2025-08-17T02:23:04,265 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.utils.util import deprecated
2025-08-17T02:23:04,265 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/utils/util.py", line 14, in <module>
2025-08-17T02:23:04,265 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     import yaml
2025-08-17T02:23:04,265 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'yaml'
2025-08-17T02:23:04,365 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:04,365 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:04,365 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:04,365 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:04,366 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:04,366 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:04,374 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change null -> WORKER_STOPPED
2025-08-17T02:23:04,374 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change null -> WORKER_STOPPED
2025-08-17T02:23:04,374 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1755422584374
2025-08-17T02:23:04,374 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1755422584374
2025-08-17T02:23:04,374 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-08-17T02:23:04,374 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-08-17T02:23:05,375 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:05,375 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:05,905 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:23:05,905 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 15, in <module>
2025-08-17T02:23:05,905 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.async_service import AsyncService
2025-08-17T02:23:05,905 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/async_service.py", line 14, in <module>
2025-08-17T02:23:05,906 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.handler_utils.utils import create_predict_response
2025-08-17T02:23:05,906 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/handler_utils/utils.py", line 5, in <module>
2025-08-17T02:23:05,906 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-08-17T02:23:05,906 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/protocol/otf_message_handler.py", line 16, in <module>
2025-08-17T02:23:05,906 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.utils.util import deprecated
2025-08-17T02:23:05,906 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/utils/util.py", line 14, in <module>
2025-08-17T02:23:05,906 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     import yaml
2025-08-17T02:23:05,906 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'yaml'
2025-08-17T02:23:06,004 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:06,004 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:06,004 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:06,004 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:06,004 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:06,004 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:06,005 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:23:06,005 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:23:06,005 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:23:06,005 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:23:06,005 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-08-17T02:23:06,005 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-08-17T02:23:07,005 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:07,005 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:07,542 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:23:07,543 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 15, in <module>
2025-08-17T02:23:07,543 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.async_service import AsyncService
2025-08-17T02:23:07,543 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/async_service.py", line 14, in <module>
2025-08-17T02:23:07,543 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.handler_utils.utils import create_predict_response
2025-08-17T02:23:07,543 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/handler_utils/utils.py", line 5, in <module>
2025-08-17T02:23:07,543 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-08-17T02:23:07,543 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/protocol/otf_message_handler.py", line 16, in <module>
2025-08-17T02:23:07,543 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.utils.util import deprecated
2025-08-17T02:23:07,543 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/utils/util.py", line 14, in <module>
2025-08-17T02:23:07,543 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     import yaml
2025-08-17T02:23:07,543 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'yaml'
2025-08-17T02:23:07,647 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:07,647 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:07,647 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:07,647 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:07,647 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:07,647 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:07,647 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:23:07,647 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:23:07,647 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:23:07,647 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:23:07,647 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-08-17T02:23:07,647 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-08-17T02:23:09,648 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:09,648 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:10,170 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:23:10,170 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 15, in <module>
2025-08-17T02:23:10,171 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.async_service import AsyncService
2025-08-17T02:23:10,171 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/async_service.py", line 14, in <module>
2025-08-17T02:23:10,171 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.handler_utils.utils import create_predict_response
2025-08-17T02:23:10,171 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/handler_utils/utils.py", line 5, in <module>
2025-08-17T02:23:10,171 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-08-17T02:23:10,171 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/protocol/otf_message_handler.py", line 16, in <module>
2025-08-17T02:23:10,171 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.utils.util import deprecated
2025-08-17T02:23:10,171 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/utils/util.py", line 14, in <module>
2025-08-17T02:23:10,171 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     import yaml
2025-08-17T02:23:10,171 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'yaml'
2025-08-17T02:23:10,270 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:10,270 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:10,270 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:10,270 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:10,270 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:10,270 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:10,270 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:23:10,270 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:23:10,270 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:23:10,270 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:23:10,270 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-08-17T02:23:10,270 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-08-17T02:23:11,390 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /127.0.0.1:37010 "PUT /predictions/phishserve HTTP/1.1" 400 1
2025-08-17T02:23:11,391 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:ashishverma-System-Product-Name,timestamp:1755422591
2025-08-17T02:23:13,271 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:13,271 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:13,801 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:23:13,802 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 15, in <module>
2025-08-17T02:23:13,802 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.async_service import AsyncService
2025-08-17T02:23:13,802 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/async_service.py", line 14, in <module>
2025-08-17T02:23:13,802 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.handler_utils.utils import create_predict_response
2025-08-17T02:23:13,802 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/handler_utils/utils.py", line 5, in <module>
2025-08-17T02:23:13,802 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-08-17T02:23:13,802 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/protocol/otf_message_handler.py", line 16, in <module>
2025-08-17T02:23:13,802 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.utils.util import deprecated
2025-08-17T02:23:13,802 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/utils/util.py", line 14, in <module>
2025-08-17T02:23:13,802 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     import yaml
2025-08-17T02:23:13,802 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'yaml'
2025-08-17T02:23:13,908 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:13,908 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:13,908 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:13,908 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:13,909 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:13,909 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:13,909 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:23:13,909 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:23:13,909 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:23:13,909 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:23:13,909 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-08-17T02:23:13,909 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-08-17T02:23:18,910 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:18,910 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:19,438 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:23:19,438 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 15, in <module>
2025-08-17T02:23:19,438 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.async_service import AsyncService
2025-08-17T02:23:19,438 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/async_service.py", line 14, in <module>
2025-08-17T02:23:19,438 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.handler_utils.utils import create_predict_response
2025-08-17T02:23:19,438 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/handler_utils/utils.py", line 5, in <module>
2025-08-17T02:23:19,438 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-08-17T02:23:19,438 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/protocol/otf_message_handler.py", line 16, in <module>
2025-08-17T02:23:19,438 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.utils.util import deprecated
2025-08-17T02:23:19,439 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/utils/util.py", line 14, in <module>
2025-08-17T02:23:19,439 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     import yaml
2025-08-17T02:23:19,439 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'yaml'
2025-08-17T02:23:19,539 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:19,539 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:19,539 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:19,539 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:19,539 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:19,539 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:19,540 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:23:19,540 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:23:19,540 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:23:19,540 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:23:19,540 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2025-08-17T02:23:19,540 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2025-08-17T02:23:27,540 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:27,540 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:28,071 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:23:28,072 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 15, in <module>
2025-08-17T02:23:28,072 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.async_service import AsyncService
2025-08-17T02:23:28,072 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/async_service.py", line 14, in <module>
2025-08-17T02:23:28,072 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.handler_utils.utils import create_predict_response
2025-08-17T02:23:28,072 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/handler_utils/utils.py", line 5, in <module>
2025-08-17T02:23:28,072 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-08-17T02:23:28,072 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/protocol/otf_message_handler.py", line 16, in <module>
2025-08-17T02:23:28,072 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.utils.util import deprecated
2025-08-17T02:23:28,072 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/utils/util.py", line 14, in <module>
2025-08-17T02:23:28,072 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     import yaml
2025-08-17T02:23:28,072 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'yaml'
2025-08-17T02:23:28,173 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:28,173 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:28,173 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:28,173 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:28,173 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:28,173 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:28,173 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:23:28,173 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:23:28,173 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:23:28,173 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:23:28,173 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2025-08-17T02:23:28,173 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2025-08-17T02:23:41,174 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:41,174 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:23:41,708 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:23:41,708 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 15, in <module>
2025-08-17T02:23:41,708 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.async_service import AsyncService
2025-08-17T02:23:41,708 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/async_service.py", line 14, in <module>
2025-08-17T02:23:41,708 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.handler_utils.utils import create_predict_response
2025-08-17T02:23:41,708 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/handler_utils/utils.py", line 5, in <module>
2025-08-17T02:23:41,708 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-08-17T02:23:41,708 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/protocol/otf_message_handler.py", line 16, in <module>
2025-08-17T02:23:41,708 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.utils.util import deprecated
2025-08-17T02:23:41,708 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/utils/util.py", line 14, in <module>
2025-08-17T02:23:41,708 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     import yaml
2025-08-17T02:23:41,708 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'yaml'
2025-08-17T02:23:41,810 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:41,810 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:23:41,810 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:41,810 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:41,810 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:23:41,810 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:23:41,810 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:23:41,810 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:23:41,810 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:23:41,810 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:23:41,810 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2025-08-17T02:23:41,810 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2025-08-17T02:23:55,989 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:phishserve,model_version:default|#hostname:ashishverma-System-Product-Name,timestamp:1755422635
2025-08-17T02:24:02,811 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:24:02,811 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:24:03,343 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:24:03,346 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 15, in <module>
2025-08-17T02:24:03,346 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.async_service import AsyncService
2025-08-17T02:24:03,346 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/async_service.py", line 14, in <module>
2025-08-17T02:24:03,346 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.handler_utils.utils import create_predict_response
2025-08-17T02:24:03,346 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/handler_utils/utils.py", line 5, in <module>
2025-08-17T02:24:03,346 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-08-17T02:24:03,346 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/protocol/otf_message_handler.py", line 16, in <module>
2025-08-17T02:24:03,346 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.utils.util import deprecated
2025-08-17T02:24:03,346 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/utils/util.py", line 14, in <module>
2025-08-17T02:24:03,346 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     import yaml
2025-08-17T02:24:03,346 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'yaml'
2025-08-17T02:24:03,445 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:24:03,445 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:24:03,445 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:24:03,445 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:24:03,445 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:24:03,445 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:24:03,446 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:24:03,446 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:24:03,446 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:24:03,446 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:24:03,446 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2025-08-17T02:24:03,446 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2025-08-17T02:24:03,851 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:24:03,851 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:24:03,884 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:24:03,884 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:24:37,446 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:24:37,446 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:24:37,969 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:24:37,969 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 15, in <module>
2025-08-17T02:24:37,969 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.async_service import AsyncService
2025-08-17T02:24:37,969 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/async_service.py", line 14, in <module>
2025-08-17T02:24:37,969 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.handler_utils.utils import create_predict_response
2025-08-17T02:24:37,969 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/handler_utils/utils.py", line 5, in <module>
2025-08-17T02:24:37,969 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-08-17T02:24:37,969 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/protocol/otf_message_handler.py", line 16, in <module>
2025-08-17T02:24:37,969 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.utils.util import deprecated
2025-08-17T02:24:37,969 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/utils/util.py", line 14, in <module>
2025-08-17T02:24:37,969 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     import yaml
2025-08-17T02:24:37,969 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'yaml'
2025-08-17T02:24:38,071 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:24:38,071 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:24:38,071 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:24:38,071 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:24:38,071 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:24:38,071 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:24:38,071 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:24:38,071 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:24:38,071 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:24:38,071 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:24:38,071 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2025-08-17T02:24:38,071 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2025-08-17T02:25:03,851 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:25:03,851 [WARN ] pool-3-thread-2 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:25:03,881 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:25:03,881 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:25:33,072 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:25:33,072 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:25:33,598 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:25:33,598 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 15, in <module>
2025-08-17T02:25:33,598 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.async_service import AsyncService
2025-08-17T02:25:33,598 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/async_service.py", line 14, in <module>
2025-08-17T02:25:33,598 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.handler_utils.utils import create_predict_response
2025-08-17T02:25:33,598 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/handler_utils/utils.py", line 5, in <module>
2025-08-17T02:25:33,598 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-08-17T02:25:33,598 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/protocol/otf_message_handler.py", line 16, in <module>
2025-08-17T02:25:33,598 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.utils.util import deprecated
2025-08-17T02:25:33,599 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/utils/util.py", line 14, in <module>
2025-08-17T02:25:33,599 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     import yaml
2025-08-17T02:25:33,599 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'yaml'
2025-08-17T02:25:33,702 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:25:33,702 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:25:33,702 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:25:33,702 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:25:33,702 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:25:33,702 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:25:33,702 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:25:33,702 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:25:33,702 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:25:33,702 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:25:33,702 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2025-08-17T02:25:33,702 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2025-08-17T02:26:03,852 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:26:03,852 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:26:03,883 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:26:03,883 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:27:02,703 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:27:02,703 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:27:03,245 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:27:03,245 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 15, in <module>
2025-08-17T02:27:03,248 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.async_service import AsyncService
2025-08-17T02:27:03,248 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/async_service.py", line 14, in <module>
2025-08-17T02:27:03,248 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.handler_utils.utils import create_predict_response
2025-08-17T02:27:03,248 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/handler_utils/utils.py", line 5, in <module>
2025-08-17T02:27:03,248 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.protocol.otf_message_handler import create_predict_response
2025-08-17T02:27:03,248 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/protocol/otf_message_handler.py", line 16, in <module>
2025-08-17T02:27:03,248 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     from ts.utils.util import deprecated
2025-08-17T02:27:03,248 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/utils/util.py", line 14, in <module>
2025-08-17T02:27:03,248 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG -     import yaml
2025-08-17T02:27:03,248 [WARN ] W-9000-phishserve_1.0-stderr MODEL_LOG - ModuleNotFoundError: No module named 'yaml'
2025-08-17T02:27:03,349 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:27:03,349 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:27:03,349 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:27:03,349 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:27:03,349 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:27:03,349 [ERROR] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorkerPython(WorkerLifeCycle.java:204) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:106) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:375) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:192) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:27:03,350 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:27:03,350 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2025-08-17T02:27:03,350 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:27:03,350 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:27:03,350 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2025-08-17T02:27:03,350 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2025-08-17T02:27:03,851 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:27:03,851 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:27:03,886 [ERROR] Thread-5 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:27:03,886 [ERROR] Thread-5 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:27:11,434 [INFO ] epollEventLoopGroup-3-3 ACCESS_LOG - /127.0.0.1:44818 "POST /predictions/phishclf HTTP/1.1" 400 0
2025-08-17T02:27:11,434 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:ashishverma-System-Product-Name,timestamp:1755422831
2025-08-17T02:28:03,852 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:28:03,852 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:28:03,888 [ERROR] Thread-6 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:28:03,888 [ERROR] Thread-6 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:29:03,851 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:29:03,851 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:29:03,886 [ERROR] Thread-7 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:29:03,886 [ERROR] Thread-7 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:30:01,535 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-08-17T02:30:01,535 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-08-17T02:30:01,537 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-08-17T02:30:01,537 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-08-17T02:30:01,556 [INFO ] main org.pytorch.serve.util.TokenAuthorization - 
######
TorchServe now enforces token authorization by default.
This requires the correct token to be provided when calling an API.
Key file located at /home/ashishverma/Documents/phishserve/key_file.json
Check token authorization documenation for information: https://github.com/pytorch/serve/blob/master/docs/token_authorization_api.md 
######

2025-08-17T02:30:01,556 [INFO ] main org.pytorch.serve.util.TokenAuthorization - 
######
TorchServe now enforces token authorization by default.
This requires the correct token to be provided when calling an API.
Key file located at /home/ashishverma/Documents/phishserve/key_file.json
Check token authorization documenation for information: https://github.com/pytorch/serve/blob/master/docs/token_authorization_api.md 
######

2025-08-17T02:30:01,557 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-08-17T02:30:01,557 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-08-17T02:30:01,577 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-08-17T02:30:01,577 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-08-17T02:30:01,641 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages
Current directory: /home/ashishverma/Documents/phishserve
Temp directory: /tmp
Metrics config path: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 20
Max heap size: 7976 M
Python executable: /home/ashishverma/Documents/phishserve/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ashishverma/Documents/phishserve/artifacts
Initial Models: phishserve.mar
Log dir: /home/ashishverma/Documents/phishserve/logs
Metrics dir: /home/ashishverma/Documents/phishserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ashishverma/Documents/phishserve/artifacts
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-08-17T02:30:01,641 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages
Current directory: /home/ashishverma/Documents/phishserve
Temp directory: /tmp
Metrics config path: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 20
Max heap size: 7976 M
Python executable: /home/ashishverma/Documents/phishserve/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ashishverma/Documents/phishserve/artifacts
Initial Models: phishserve.mar
Log dir: /home/ashishverma/Documents/phishserve/logs
Metrics dir: /home/ashishverma/Documents/phishserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ashishverma/Documents/phishserve/artifacts
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-08-17T02:30:01,645 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-08-17T02:30:01,645 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-08-17T02:30:01,646 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: phishserve.mar
2025-08-17T02:30:01,646 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: phishserve.mar
2025-08-17T02:30:01,681 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model phishserve
2025-08-17T02:30:01,681 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model phishserve
2025-08-17T02:30:01,681 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model phishserve
2025-08-17T02:30:01,681 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model phishserve
2025-08-17T02:30:01,681 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model phishserve loaded.
2025-08-17T02:30:01,681 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model phishserve loaded.
2025-08-17T02:30:01,681 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: phishserve, count: 1
2025-08-17T02:30:01,681 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: phishserve, count: 1
2025-08-17T02:30:01,684 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-08-17T02:30:01,684 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-08-17T02:30:01,685 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:30:01,685 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:30:01,707 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-08-17T02:30:01,707 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-08-17T02:30:01,707 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-08-17T02:30:01,707 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-08-17T02:30:01,708 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-08-17T02:30:01,708 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-08-17T02:30:01,708 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-08-17T02:30:01,708 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-08-17T02:30:01,708 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-08-17T02:30:01,708 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-08-17T02:30:01,792 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:30:01,792 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:30:01,820 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:30:01,820 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:30:02,265 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=18998
2025-08-17T02:30:02,266 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-08-17T02:30:02,270 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Successfully loaded /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-08-17T02:30:02,270 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - [PID]18998
2025-08-17T02:30:02,270 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch worker started.
2025-08-17T02:30:02,270 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-08-17T02:30:02,270 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change null -> WORKER_STARTED
2025-08-17T02:30:02,270 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change null -> WORKER_STARTED
2025-08-17T02:30:02,273 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:30:02,273 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:30:02,278 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-08-17T02:30:02,279 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423002279
2025-08-17T02:30:02,279 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423002279
2025-08-17T02:30:02,280 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423002280
2025-08-17T02:30:02,280 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423002280
2025-08-17T02:30:02,293 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - model_name: phishserve, batchSize: 1
2025-08-17T02:30:02,402 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-08-17T02:30:02,402 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-08-17T02:30:02,402 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-08-17T02:30:02,402 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-08-17T02:30:02,484 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Backend worker process died.
2025-08-17T02:30:02,484 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:30:02,484 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-08-17T02:30:02,484 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     worker.run_server()
2025-08-17T02:30:02,484 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-08-17T02:30:02,484 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-08-17T02:30:02,484 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 196, in handle_connection
2025-08-17T02:30:02,484 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-08-17T02:30:02,484 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-08-17T02:30:02,485 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:02,484 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-08-17T02:30:02,485 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-08-17T02:30:02,485 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-08-17T02:30:02,485 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:02,485 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-08-17T02:30:02,485 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-08-17T02:30:02,485 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-08-17T02:30:02,485 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-08-17T02:30:02,485 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/tmp/models/db14d21b8f18488c81b8e6e9bc7acd3a/handler.py", line 30, in initialize
2025-08-17T02:30:02,485 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     self.model = self.load_model(model_dir)
2025-08-17T02:30:02,485 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:02,485 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/tmp/models/db14d21b8f18488c81b8e6e9bc7acd3a/handler.py", line 48, in load_model
2025-08-17T02:30:02,485 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     model = PhishingClassifier(vocab_size=len(self.itos), emb_dim=ck["emb_dim"], hid=ck["hid"], num_classes=2, pad_idx=self.PAD)
2025-08-17T02:30:02,485 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                                                                                                                        ^^^^^^^^
2025-08-17T02:30:02,485 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - AttributeError: 'PhishHandler' object has no attribute 'PAD'
2025-08-17T02:30:02,485 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) ~[model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:30:02,485 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) ~[model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:30:02,491 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: phishserve, error: Worker died.
2025-08-17T02:30:02,491 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: phishserve, error: Worker died.
2025-08-17T02:30:02,492 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-08-17T02:30:02,492 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-08-17T02:30:02,492 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1755423002492
2025-08-17T02:30:02,492 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1755423002492
2025-08-17T02:30:02,492 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-08-17T02:30:02,492 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-08-17T02:30:02,534 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:30:02,534 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:30:02,534 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:30:02,534 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:30:03,493 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:30:03,493 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:30:04,033 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=19052
2025-08-17T02:30:04,033 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-08-17T02:30:04,035 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Successfully loaded /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-08-17T02:30:04,035 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - [PID]19052
2025-08-17T02:30:04,035 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch worker started.
2025-08-17T02:30:04,035 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-08-17T02:30:04,036 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-08-17T02:30:04,036 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-08-17T02:30:04,036 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:30:04,036 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:30:04,037 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-08-17T02:30:04,037 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423004037
2025-08-17T02:30:04,037 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423004037
2025-08-17T02:30:04,037 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423004037
2025-08-17T02:30:04,037 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423004037
2025-08-17T02:30:04,047 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - model_name: phishserve, batchSize: 1
2025-08-17T02:30:04,105 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-08-17T02:30:04,106 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-08-17T02:30:04,106 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-08-17T02:30:04,106 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-08-17T02:30:04,176 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Backend worker process died.
2025-08-17T02:30:04,176 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-08-17T02:30:04,176 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:30:04,176 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-08-17T02:30:04,176 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-08-17T02:30:04,176 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     worker.run_server()
2025-08-17T02:30:04,176 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-08-17T02:30:04,176 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-08-17T02:30:04,176 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-08-17T02:30:04,176 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-08-17T02:30:04,176 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 196, in handle_connection
2025-08-17T02:30:04,176 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-08-17T02:30:04,176 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:04,176 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-08-17T02:30:04,176 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:30:04,176 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-08-17T02:30:04,177 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:04,176 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:30:04,177 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-08-17T02:30:04,177 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: phishserve, error: Worker died.
2025-08-17T02:30:04,177 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-08-17T02:30:04,177 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: phishserve, error: Worker died.
2025-08-17T02:30:04,177 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/tmp/models/db14d21b8f18488c81b8e6e9bc7acd3a/handler.py", line 30, in initialize
2025-08-17T02:30:04,177 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-08-17T02:30:04,177 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     self.model = self.load_model(model_dir)
2025-08-17T02:30:04,177 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-08-17T02:30:04,177 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:04,177 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:30:04,177 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/tmp/models/db14d21b8f18488c81b8e6e9bc7acd3a/handler.py", line 48, in load_model
2025-08-17T02:30:04,177 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:30:04,177 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     model = PhishingClassifier(vocab_size=len(self.itos), emb_dim=ck["emb_dim"], hid=ck["hid"], num_classes=2, pad_idx=self.PAD)
2025-08-17T02:30:04,177 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                                                                                                                        ^^^^^^^^
2025-08-17T02:30:04,177 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - AttributeError: 'PhishHandler' object has no attribute 'PAD'
2025-08-17T02:30:04,177 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-08-17T02:30:04,177 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:30:04,177 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:30:04,177 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2025-08-17T02:30:04,224 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:30:04,224 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:30:05,178 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:30:05,178 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:30:05,699 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /127.0.0.1:43448 "PUT /predictions/phishserve HTTP/1.1" 400 1
2025-08-17T02:30:05,699 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:ashishverma-System-Product-Name,timestamp:1755423005
2025-08-17T02:30:05,719 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=19089
2025-08-17T02:30:05,720 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-08-17T02:30:05,722 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Successfully loaded /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-08-17T02:30:05,722 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - [PID]19089
2025-08-17T02:30:05,722 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch worker started.
2025-08-17T02:30:05,722 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-08-17T02:30:05,722 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-08-17T02:30:05,722 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-08-17T02:30:05,722 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:30:05,722 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:30:05,723 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423005723
2025-08-17T02:30:05,723 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-08-17T02:30:05,723 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423005723
2025-08-17T02:30:05,723 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423005723
2025-08-17T02:30:05,723 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423005723
2025-08-17T02:30:05,731 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - model_name: phishserve, batchSize: 1
2025-08-17T02:30:05,817 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-08-17T02:30:05,817 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-08-17T02:30:05,817 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-08-17T02:30:05,817 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Backend worker process died.
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     worker.run_server()
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 196, in handle_connection
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-08-17T02:30:05,897 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-08-17T02:30:05,897 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/tmp/models/db14d21b8f18488c81b8e6e9bc7acd3a/handler.py", line 30, in initialize
2025-08-17T02:30:05,897 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-08-17T02:30:05,897 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     self.model = self.load_model(model_dir)
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/tmp/models/db14d21b8f18488c81b8e6e9bc7acd3a/handler.py", line 48, in load_model
2025-08-17T02:30:05,897 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:30:05,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     model = PhishingClassifier(vocab_size=len(self.itos), emb_dim=ck["emb_dim"], hid=ck["hid"], num_classes=2, pad_idx=self.PAD)
2025-08-17T02:30:05,898 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                                                                                                                        ^^^^^^^^
2025-08-17T02:30:05,897 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:30:05,898 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - AttributeError: 'PhishHandler' object has no attribute 'PAD'
2025-08-17T02:30:05,898 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: phishserve, error: Worker died.
2025-08-17T02:30:05,898 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: phishserve, error: Worker died.
2025-08-17T02:30:05,898 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-08-17T02:30:05,898 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-08-17T02:30:05,898 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:30:05,898 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:30:05,898 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-08-17T02:30:05,898 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2025-08-17T02:30:05,943 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:30:05,943 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:30:05,943 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:30:05,943 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:30:07,899 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:30:07,899 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:30:08,446 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=19138
2025-08-17T02:30:08,446 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-08-17T02:30:08,449 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Successfully loaded /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-08-17T02:30:08,449 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - [PID]19138
2025-08-17T02:30:08,449 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch worker started.
2025-08-17T02:30:08,449 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-08-17T02:30:08,449 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-08-17T02:30:08,449 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-08-17T02:30:08,449 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:30:08,449 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:30:08,450 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423008450
2025-08-17T02:30:08,450 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423008450
2025-08-17T02:30:08,450 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423008450
2025-08-17T02:30:08,450 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423008450
2025-08-17T02:30:08,450 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-08-17T02:30:08,458 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - model_name: phishserve, batchSize: 1
2025-08-17T02:30:08,542 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-08-17T02:30:08,542 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-08-17T02:30:08,542 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-08-17T02:30:08,542 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Backend worker process died.
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     worker.run_server()
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-08-17T02:30:08,619 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 196, in handle_connection
2025-08-17T02:30:08,619 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-08-17T02:30:08,619 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-08-17T02:30:08,619 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/tmp/models/db14d21b8f18488c81b8e6e9bc7acd3a/handler.py", line 30, in initialize
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     self.model = self.load_model(model_dir)
2025-08-17T02:30:08,619 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:08,619 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/tmp/models/db14d21b8f18488c81b8e6e9bc7acd3a/handler.py", line 48, in load_model
2025-08-17T02:30:08,619 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: phishserve, error: Worker died.
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     model = PhishingClassifier(vocab_size=len(self.itos), emb_dim=ck["emb_dim"], hid=ck["hid"], num_classes=2, pad_idx=self.PAD)
2025-08-17T02:30:08,619 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: phishserve, error: Worker died.
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                                                                                                                        ^^^^^^^^
2025-08-17T02:30:08,619 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-08-17T02:30:08,619 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-08-17T02:30:08,619 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:30:08,619 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:30:08,619 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - AttributeError: 'PhishHandler' object has no attribute 'PAD'
2025-08-17T02:30:08,620 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-08-17T02:30:08,620 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2025-08-17T02:30:08,663 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:30:08,663 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:30:08,663 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:30:08,663 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:30:11,620 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:30:11,620 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:30:12,163 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=19175
2025-08-17T02:30:12,163 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-08-17T02:30:12,166 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Successfully loaded /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-08-17T02:30:12,166 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - [PID]19175
2025-08-17T02:30:12,166 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch worker started.
2025-08-17T02:30:12,166 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-08-17T02:30:12,166 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-08-17T02:30:12,166 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-08-17T02:30:12,166 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:30:12,166 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:30:12,167 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-08-17T02:30:12,167 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423012167
2025-08-17T02:30:12,167 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423012167
2025-08-17T02:30:12,167 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423012167
2025-08-17T02:30:12,167 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423012167
2025-08-17T02:30:12,179 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - model_name: phishserve, batchSize: 1
2025-08-17T02:30:12,264 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-08-17T02:30:12,264 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-08-17T02:30:12,264 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-08-17T02:30:12,264 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Backend worker process died.
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     worker.run_server()
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 196, in handle_connection
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/tmp/models/db14d21b8f18488c81b8e6e9bc7acd3a/handler.py", line 30, in initialize
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     self.model = self.load_model(model_dir)
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:12,342 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/tmp/models/db14d21b8f18488c81b8e6e9bc7acd3a/handler.py", line 48, in load_model
2025-08-17T02:30:12,343 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     model = PhishingClassifier(vocab_size=len(self.itos), emb_dim=ck["emb_dim"], hid=ck["hid"], num_classes=2, pad_idx=self.PAD)
2025-08-17T02:30:12,342 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-08-17T02:30:12,343 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                                                                                                                        ^^^^^^^^
2025-08-17T02:30:12,343 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - AttributeError: 'PhishHandler' object has no attribute 'PAD'
2025-08-17T02:30:12,342 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-08-17T02:30:12,343 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-08-17T02:30:12,343 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-08-17T02:30:12,343 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:30:12,343 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:30:12,343 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: phishserve, error: Worker died.
2025-08-17T02:30:12,343 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: phishserve, error: Worker died.
2025-08-17T02:30:12,343 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-08-17T02:30:12,343 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-08-17T02:30:12,343 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:30:12,343 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:30:12,343 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-08-17T02:30:12,343 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2025-08-17T02:30:12,398 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:30:12,398 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:30:12,398 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:30:12,398 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:30:16,221 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /127.0.0.1:38932 "POST /predictions/phishclf HTTP/1.1" 400 0
2025-08-17T02:30:16,221 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:ashishverma-System-Product-Name,timestamp:1755423016
2025-08-17T02:30:17,344 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:30:17,344 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:30:17,891 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=19244
2025-08-17T02:30:17,891 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-08-17T02:30:17,894 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Successfully loaded /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-08-17T02:30:17,894 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - [PID]19244
2025-08-17T02:30:17,894 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch worker started.
2025-08-17T02:30:17,894 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-08-17T02:30:17,894 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-08-17T02:30:17,894 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2025-08-17T02:30:17,894 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:30:17,894 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:30:17,895 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-08-17T02:30:17,895 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423017895
2025-08-17T02:30:17,895 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423017895
2025-08-17T02:30:17,896 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423017896
2025-08-17T02:30:17,896 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423017896
2025-08-17T02:30:17,903 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - model_name: phishserve, batchSize: 1
2025-08-17T02:30:18,002 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-08-17T02:30:18,002 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-08-17T02:30:18,002 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-08-17T02:30:18,002 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-08-17T02:30:18,084 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Backend worker process died.
2025-08-17T02:30:18,084 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2025-08-17T02:30:18,084 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 301, in <module>
2025-08-17T02:30:18,084 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     worker.run_server()
2025-08-17T02:30:18,084 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 268, in run_server
2025-08-17T02:30:18,084 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2025-08-17T02:30:18,084 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 196, in handle_connection
2025-08-17T02:30:18,084 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2025-08-17T02:30:18,084 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-08-17T02:30:18,084 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:18,084 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py", line 133, in load_model
2025-08-17T02:30:18,084 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2025-08-17T02:30:18,084 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     service = model_loader.load(
2025-08-17T02:30:18,085 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:18,085 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_loader.py", line 143, in load
2025-08-17T02:30:18,085 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2025-08-17T02:30:18,085 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/tmp/models/db14d21b8f18488c81b8e6e9bc7acd3a/handler.py", line 30, in initialize
2025-08-17T02:30:18,085 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     self.model = self.load_model(model_dir)
2025-08-17T02:30:18,085 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-08-17T02:30:18,085 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-08-17T02:30:18,085 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -   File "/tmp/models/db14d21b8f18488c81b8e6e9bc7acd3a/handler.py", line 48, in load_model
2025-08-17T02:30:18,085 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2025-08-17T02:30:18,085 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -     model = PhishingClassifier(vocab_size=len(self.itos), emb_dim=ck["emb_dim"], hid=ck["hid"], num_classes=2, pad_idx=self.PAD)
2025-08-17T02:30:18,085 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG -                                                                                                                        ^^^^^^^^
2025-08-17T02:30:18,085 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - AttributeError: 'PhishHandler' object has no attribute 'PAD'
2025-08-17T02:30:18,085 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:30:18,085 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1770) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) ~[model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2025-08-17T02:30:18,085 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: phishserve, error: Worker died.
2025-08-17T02:30:18,085 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: phishserve, error: Worker died.
2025-08-17T02:30:18,085 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-08-17T02:30:18,085 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2025-08-17T02:30:18,085 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:30:18,085 [WARN ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2025-08-17T02:30:18,085 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2025-08-17T02:30:18,085 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2025-08-17T02:30:18,131 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:30:18,131 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stdout
2025-08-17T02:30:18,131 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:30:18,131 [INFO ] W-9000-phishserve_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-phishserve_1.0-stderr
2025-08-17T02:30:54,193 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-08-17T02:30:54,193 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-08-17T02:30:54,195 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-08-17T02:30:54,195 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-08-17T02:30:54,215 [INFO ] main org.pytorch.serve.util.TokenAuthorization - 
######
TorchServe now enforces token authorization by default.
This requires the correct token to be provided when calling an API.
Key file located at /home/ashishverma/Documents/phishserve/key_file.json
Check token authorization documenation for information: https://github.com/pytorch/serve/blob/master/docs/token_authorization_api.md 
######

2025-08-17T02:30:54,215 [INFO ] main org.pytorch.serve.util.TokenAuthorization - 
######
TorchServe now enforces token authorization by default.
This requires the correct token to be provided when calling an API.
Key file located at /home/ashishverma/Documents/phishserve/key_file.json
Check token authorization documenation for information: https://github.com/pytorch/serve/blob/master/docs/token_authorization_api.md 
######

2025-08-17T02:30:54,215 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-08-17T02:30:54,215 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-08-17T02:30:54,236 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-08-17T02:30:54,236 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-08-17T02:30:54,292 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages
Current directory: /home/ashishverma/Documents/phishserve
Temp directory: /tmp
Metrics config path: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 20
Max heap size: 7976 M
Python executable: /home/ashishverma/Documents/phishserve/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ashishverma/Documents/phishserve/artifacts
Initial Models: phishserve.mar
Log dir: /home/ashishverma/Documents/phishserve/logs
Metrics dir: /home/ashishverma/Documents/phishserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ashishverma/Documents/phishserve/artifacts
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-08-17T02:30:54,292 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages
Current directory: /home/ashishverma/Documents/phishserve
Temp directory: /tmp
Metrics config path: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 20
Max heap size: 7976 M
Python executable: /home/ashishverma/Documents/phishserve/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ashishverma/Documents/phishserve/artifacts
Initial Models: phishserve.mar
Log dir: /home/ashishverma/Documents/phishserve/logs
Metrics dir: /home/ashishverma/Documents/phishserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ashishverma/Documents/phishserve/artifacts
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-08-17T02:30:54,296 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-08-17T02:30:54,296 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-08-17T02:30:54,297 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: phishserve.mar
2025-08-17T02:30:54,297 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: phishserve.mar
2025-08-17T02:30:54,330 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model phishserve
2025-08-17T02:30:54,330 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model phishserve
2025-08-17T02:30:54,330 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model phishserve
2025-08-17T02:30:54,330 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model phishserve
2025-08-17T02:30:54,330 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model phishserve loaded.
2025-08-17T02:30:54,330 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model phishserve loaded.
2025-08-17T02:30:54,330 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: phishserve, count: 1
2025-08-17T02:30:54,330 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: phishserve, count: 1
2025-08-17T02:30:54,334 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-08-17T02:30:54,334 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-08-17T02:30:54,334 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:30:54,334 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:30:54,357 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-08-17T02:30:54,357 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-08-17T02:30:54,358 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-08-17T02:30:54,358 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-08-17T02:30:54,358 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-08-17T02:30:54,358 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-08-17T02:30:54,358 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-08-17T02:30:54,358 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-08-17T02:30:54,358 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-08-17T02:30:54,358 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-08-17T02:30:54,446 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:30:54,446 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:30:54,478 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:30:54,478 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:30:54,897 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=19611
2025-08-17T02:30:54,898 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-08-17T02:30:54,900 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Successfully loaded /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-08-17T02:30:54,901 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - [PID]19611
2025-08-17T02:30:54,901 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch worker started.
2025-08-17T02:30:54,901 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-08-17T02:30:54,901 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change null -> WORKER_STARTED
2025-08-17T02:30:54,901 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change null -> WORKER_STARTED
2025-08-17T02:30:54,904 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:30:54,904 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:30:54,908 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-08-17T02:30:54,910 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423054910
2025-08-17T02:30:54,910 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423054910
2025-08-17T02:30:54,910 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423054910
2025-08-17T02:30:54,910 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423054910
2025-08-17T02:30:54,926 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - model_name: phishserve, batchSize: 1
2025-08-17T02:30:55,023 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-08-17T02:30:55,023 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-08-17T02:30:55,023 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-08-17T02:30:55,023 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-08-17T02:30:55,125 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 214
2025-08-17T02:30:55,125 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 214
2025-08-17T02:30:55,125 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-08-17T02:30:55,125 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-08-17T02:30:55,125 [INFO ] W-9000-phishserve_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:793.0|#WorkerName:W-9000-phishserve_1.0,Level:Host|#hostname:ashishverma-System-Product-Name,timestamp:1755423055
2025-08-17T02:30:55,125 [INFO ] W-9000-phishserve_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:ashishverma-System-Product-Name,timestamp:1755423055
2025-08-17T02:31:01,594 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /127.0.0.1:50810 "PUT /predictions/phishserve HTTP/1.1" 400 1
2025-08-17T02:31:01,594 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:ashishverma-System-Product-Name,timestamp:1755423061
2025-08-17T02:31:17,140 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:phishserve,model_version:default|#hostname:ashishverma-System-Product-Name,timestamp:1755423077
2025-08-17T02:31:17,141 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1755423077141
2025-08-17T02:31:17,141 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1755423077141
2025-08-17T02:31:17,142 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423077142
2025-08-17T02:31:17,142 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423077142
2025-08-17T02:31:17,142 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Backend received inference at: 1755423077
2025-08-17T02:31:17,209 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:66.62|#ModelName:phishserve,Level:Model|#type:GAUGE|#hostname:ashishverma-System-Product-Name,1755423077,135e73de-032e-454c-974f-6a849557bda0, pattern=[METRICS]
2025-08-17T02:31:17,209 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:66.62|#ModelName:phishserve,Level:Model|#type:GAUGE|#hostname:ashishverma-System-Product-Name,1755423077,135e73de-032e-454c-974f-6a849557bda0, pattern=[METRICS]
2025-08-17T02:31:17,209 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId 135e73de-032e-454c-974f-6a849557bda0
2025-08-17T02:31:17,209 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId 135e73de-032e-454c-974f-6a849557bda0
2025-08-17T02:31:17,210 [INFO ] W-9000-phishserve_1.0 ACCESS_LOG - /127.0.0.1:45328 "PUT /predictions/phishserve HTTP/1.1" 200 69
2025-08-17T02:31:17,210 [INFO ] W-9000-phishserve_1.0-stdout MODEL_METRICS - HandlerTime.ms:66.62|#ModelName:phishserve,Level:Model|#hostname:ashishverma-System-Product-Name,requestID:135e73de-032e-454c-974f-6a849557bda0,timestamp:1755423077
2025-08-17T02:31:17,210 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:66.72|#ModelName:phishserve,Level:Model|#type:GAUGE|#hostname:ashishverma-System-Product-Name,1755423077,135e73de-032e-454c-974f-6a849557bda0, pattern=[METRICS]
2025-08-17T02:31:17,210 [INFO ] W-9000-phishserve_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:ashishverma-System-Product-Name,timestamp:1755423077
2025-08-17T02:31:17,210 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:66.72|#ModelName:phishserve,Level:Model|#type:GAUGE|#hostname:ashishverma-System-Product-Name,1755423077,135e73de-032e-454c-974f-6a849557bda0, pattern=[METRICS]
2025-08-17T02:31:17,210 [INFO ] W-9000-phishserve_1.0-stdout MODEL_METRICS - PredictionTime.ms:66.72|#ModelName:phishserve,Level:Model|#hostname:ashishverma-System-Product-Name,requestID:135e73de-032e-454c-974f-6a849557bda0,timestamp:1755423077
2025-08-17T02:31:17,210 [INFO ] W-9000-phishserve_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:68192.329|#model_name:phishserve,model_version:default|#hostname:ashishverma-System-Product-Name,timestamp:1755423077
2025-08-17T02:31:17,210 [INFO ] W-9000-phishserve_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:73.942|#model_name:phishserve,model_version:default|#hostname:ashishverma-System-Product-Name,timestamp:1755423077
2025-08-17T02:31:17,210 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 73942, Backend time ns: 68665192
2025-08-17T02:31:17,210 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 73942, Backend time ns: 68665192
2025-08-17T02:31:17,210 [INFO ] W-9000-phishserve_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:ashishverma-System-Product-Name,timestamp:1755423077
2025-08-17T02:31:17,210 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67
2025-08-17T02:31:17,210 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67
2025-08-17T02:31:17,210 [INFO ] W-9000-phishserve_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:ashishverma-System-Product-Name,timestamp:1755423077
2025-08-17T02:34:02,973 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-08-17T02:34:02,973 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, No such file or directory
2025-08-17T02:34:02,975 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-08-17T02:34:02,975 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-08-17T02:34:02,993 [INFO ] main org.pytorch.serve.util.TokenAuthorization - 
######
TorchServe now enforces token authorization by default.
This requires the correct token to be provided when calling an API.
Key file located at /home/ashishverma/Documents/phishserve/key_file.json
Check token authorization documenation for information: https://github.com/pytorch/serve/blob/master/docs/token_authorization_api.md 
######

2025-08-17T02:34:02,993 [INFO ] main org.pytorch.serve.util.TokenAuthorization - 
######
TorchServe now enforces token authorization by default.
This requires the correct token to be provided when calling an API.
Key file located at /home/ashishverma/Documents/phishserve/key_file.json
Check token authorization documenation for information: https://github.com/pytorch/serve/blob/master/docs/token_authorization_api.md 
######

2025-08-17T02:34:02,994 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-08-17T02:34:02,994 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-08-17T02:34:03,013 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-08-17T02:34:03,013 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
2025-08-17T02:34:03,078 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages
Current directory: /home/ashishverma/Documents/phishserve
Temp directory: /tmp
Metrics config path: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 20
Max heap size: 7976 M
Python executable: /home/ashishverma/Documents/phishserve/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ashishverma/Documents/phishserve/artifacts
Initial Models: phishserve.mar
Log dir: /home/ashishverma/Documents/phishserve/logs
Metrics dir: /home/ashishverma/Documents/phishserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ashishverma/Documents/phishserve/artifacts
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-08-17T02:34:03,078 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages
Current directory: /home/ashishverma/Documents/phishserve
Temp directory: /tmp
Metrics config path: /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 20
Max heap size: 7976 M
Python executable: /home/ashishverma/Documents/phishserve/.venv/bin/python
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ashishverma/Documents/phishserve/artifacts
Initial Models: phishserve.mar
Log dir: /home/ashishverma/Documents/phishserve/logs
Metrics dir: /home/ashishverma/Documents/phishserve/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ashishverma/Documents/phishserve/artifacts
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-08-17T02:34:03,082 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-08-17T02:34:03,082 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-08-17T02:34:03,082 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: phishserve.mar
2025-08-17T02:34:03,082 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: phishserve.mar
2025-08-17T02:34:03,114 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model phishserve
2025-08-17T02:34:03,114 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model phishserve
2025-08-17T02:34:03,114 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model phishserve
2025-08-17T02:34:03,114 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model phishserve
2025-08-17T02:34:03,114 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model phishserve loaded.
2025-08-17T02:34:03,114 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model phishserve loaded.
2025-08-17T02:34:03,114 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: phishserve, count: 1
2025-08-17T02:34:03,114 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: phishserve, count: 1
2025-08-17T02:34:03,117 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-08-17T02:34:03,117 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-08-17T02:34:03,118 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:34:03,118 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/ashishverma/Documents/phishserve/.venv/bin/python, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml]
2025-08-17T02:34:03,140 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-08-17T02:34:03,140 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2025-08-17T02:34:03,141 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-08-17T02:34:03,141 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-08-17T02:34:03,141 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-08-17T02:34:03,141 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2025-08-17T02:34:03,141 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-08-17T02:34:03,141 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2025-08-17T02:34:03,141 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-08-17T02:34:03,141 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2025-08-17T02:34:03,223 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:34:03,223 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2025-08-17T02:34:03,255 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:34:03,255 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:34:03,656 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=20622
2025-08-17T02:34:03,657 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-08-17T02:34:03,658 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Successfully loaded /home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/configs/metrics.yaml.
2025-08-17T02:34:03,659 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - [PID]20622
2025-08-17T02:34:03,659 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch worker started.
2025-08-17T02:34:03,659 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Python runtime: 3.12.3
2025-08-17T02:34:03,659 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change null -> WORKER_STARTED
2025-08-17T02:34:03,659 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change null -> WORKER_STARTED
2025-08-17T02:34:03,661 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:34:03,661 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-08-17T02:34:03,665 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-08-17T02:34:03,666 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423243666
2025-08-17T02:34:03,666 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1755423243666
2025-08-17T02:34:03,667 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423243667
2025-08-17T02:34:03,667 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423243667
2025-08-17T02:34:03,679 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - model_name: phishserve, batchSize: 1
2025-08-17T02:34:03,789 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Enabled tensor cores
2025-08-17T02:34:03,790 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2025-08-17T02:34:03,790 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2025-08-17T02:34:03,790 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2025-08-17T02:34:03,895 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 228
2025-08-17T02:34:03,895 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 228
2025-08-17T02:34:03,895 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-08-17T02:34:03,895 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-phishserve_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2025-08-17T02:34:03,895 [INFO ] W-9000-phishserve_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:779.0|#WorkerName:W-9000-phishserve_1.0,Level:Host|#hostname:ashishverma-System-Product-Name,timestamp:1755423243
2025-08-17T02:34:03,896 [INFO ] W-9000-phishserve_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:ashishverma-System-Product-Name,timestamp:1755423243
2025-08-17T02:34:41,642 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:phishserve,model_version:default|#hostname:ashishverma-System-Product-Name,timestamp:1755423281
2025-08-17T02:34:41,643 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1755423281643
2025-08-17T02:34:41,643 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1755423281643
2025-08-17T02:34:41,643 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423281643
2025-08-17T02:34:41,643 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1755423281643
2025-08-17T02:34:41,644 [INFO ] W-9000-phishserve_1.0-stdout MODEL_LOG - Backend received inference at: 1755423281
2025-08-17T02:34:41,713 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:68.68|#ModelName:phishserve,Level:Model|#type:GAUGE|#hostname:ashishverma-System-Product-Name,1755423281,01eeddae-f138-463c-9615-5b3c7e22ca6f, pattern=[METRICS]
2025-08-17T02:34:41,713 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:68.68|#ModelName:phishserve,Level:Model|#type:GAUGE|#hostname:ashishverma-System-Product-Name,1755423281,01eeddae-f138-463c-9615-5b3c7e22ca6f, pattern=[METRICS]
2025-08-17T02:34:41,713 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId 01eeddae-f138-463c-9615-5b3c7e22ca6f
2025-08-17T02:34:41,713 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId 01eeddae-f138-463c-9615-5b3c7e22ca6f
2025-08-17T02:34:41,713 [INFO ] W-9000-phishserve_1.0-stdout MODEL_METRICS - HandlerTime.ms:68.68|#ModelName:phishserve,Level:Model|#hostname:ashishverma-System-Product-Name,requestID:01eeddae-f138-463c-9615-5b3c7e22ca6f,timestamp:1755423281
2025-08-17T02:34:41,713 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:68.78|#ModelName:phishserve,Level:Model|#type:GAUGE|#hostname:ashishverma-System-Product-Name,1755423281,01eeddae-f138-463c-9615-5b3c7e22ca6f, pattern=[METRICS]
2025-08-17T02:34:41,713 [INFO ] W-9000-phishserve_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:68.78|#ModelName:phishserve,Level:Model|#type:GAUGE|#hostname:ashishverma-System-Product-Name,1755423281,01eeddae-f138-463c-9615-5b3c7e22ca6f, pattern=[METRICS]
2025-08-17T02:34:41,713 [INFO ] W-9000-phishserve_1.0 ACCESS_LOG - /127.0.0.1:35100 "PUT /predictions/phishserve HTTP/1.1" 200 72
2025-08-17T02:34:41,714 [INFO ] W-9000-phishserve_1.0-stdout MODEL_METRICS - PredictionTime.ms:68.78|#ModelName:phishserve,Level:Model|#hostname:ashishverma-System-Product-Name,requestID:01eeddae-f138-463c-9615-5b3c7e22ca6f,timestamp:1755423281
2025-08-17T02:34:41,714 [INFO ] W-9000-phishserve_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:ashishverma-System-Product-Name,timestamp:1755423281
2025-08-17T02:34:41,714 [INFO ] W-9000-phishserve_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:70433.678|#model_name:phishserve,model_version:default|#hostname:ashishverma-System-Product-Name,timestamp:1755423281
2025-08-17T02:34:41,714 [INFO ] W-9000-phishserve_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:115.185|#model_name:phishserve,model_version:default|#hostname:ashishverma-System-Product-Name,timestamp:1755423281
2025-08-17T02:34:41,714 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 115185, Backend time ns: 70977540
2025-08-17T02:34:41,714 [DEBUG] W-9000-phishserve_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 115185, Backend time ns: 70977540
2025-08-17T02:34:41,714 [INFO ] W-9000-phishserve_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:ashishverma-System-Product-Name,timestamp:1755423281
2025-08-17T02:34:41,714 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 70
2025-08-17T02:34:41,714 [INFO ] W-9000-phishserve_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 70
2025-08-17T02:34:41,714 [INFO ] W-9000-phishserve_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:ashishverma-System-Product-Name,timestamp:1755423281
2025-08-17T02:35:03,251 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2025-08-17T02:35:03,251 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ashishverma/Documents/phishserve/.venv/lib/python3.12/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

